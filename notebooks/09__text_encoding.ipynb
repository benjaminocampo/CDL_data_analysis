{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encoding\n",
    "\n",
    "Bag of Words (BoW):\n",
    "\n",
    "The Bag of Words (BoW) model is one of the simplest methods of text encoding. Here's how it works:\n",
    "\n",
    "- Vocabulary Creation:\n",
    "    A vocabulary is created by listing all the unique words in the text corpus.\n",
    "    Commonly, preprocessing steps like removing punctuation, converting to lowercase, and stemming/lemmatization are applied to standardize the text and reduce the vocabulary size.\n",
    "\n",
    "- Text Vectorization:\n",
    "    Each document/text is represented as a vector in a multi-dimensional space, where each dimension corresponds to a term (word) in the vocabulary.\n",
    "    The value in each dimension is the frequency of that term in the document.\n",
    "\n",
    "For example, consider two documents:\n",
    "\n",
    "    Doc1: \"I love programming.\"\n",
    "    Doc2: \"Programming is fun.\"\n",
    "\n",
    "The vocabulary will be:\n",
    "\n",
    "    ['I', 'love', 'programming', 'is', 'fun']\n",
    "\n",
    "The BoW representations will be:\n",
    "\n",
    "    BoW(Doc1) = [1, 1, 1, 0, 0]\n",
    "    BoW(Doc2) = [0, 0, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>data</th>\n",
       "      <th>field</th>\n",
       "      <th>interesting</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>of</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>science</th>\n",
       "      <th>subset</th>\n",
       "      <th>versatile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  data  field  interesting  is  language  learning  love  machine  of  \\\n",
       "0   0     0      0            0   0         0         0     1        0   0   \n",
       "1   0     0      0            0   1         1         0     0        0   0   \n",
       "2   1     1      1            1   1         0         0     0        0   0   \n",
       "3   0     1      0            0   1         0         1     0        1   1   \n",
       "\n",
       "   programming  python  science  subset  versatile  \n",
       "0            1       0        0       0          0  \n",
       "1            0       1        0       0          1  \n",
       "2            0       0        1       0          0  \n",
       "3            0       0        1       1          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Toy text data\n",
    "documents = [\n",
    "    'I love programming.',\n",
    "    'Python is a versatile language.',\n",
    "    'Data science is an interesting field.',\n",
    "    'Machine learning is a subset of data science.'\n",
    "]\n",
    "\n",
    "# -------------------\n",
    "# Bag of Words\n",
    "# -------------------\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform(documents)\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is a bit more complex and attempts to take into account not just the occurrence of terms in a single document (Term Frequency, TF), but also how unique the terms are across the entire corpus (Inverse Document Frequency, IDF).\n",
    "\n",
    "**Term Frequency (TF)**\n",
    "    Like in BoW, TF is the frequency of a term in a document. It's calculated similarly.\n",
    "    \n",
    "    TF(t)=Number of times term t appears in a document\n",
    "\n",
    "**Inverse Document Frequency (IDF)**\n",
    "    IDF measures the importance of a term in the corpus.\n",
    "    \n",
    "    IDF(t)=log(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "**TF-IDF Score**\n",
    "    The TF-IDF score for a term is the product of its TF and IDF scores.\n",
    "    \n",
    "    TF-IDF(t)=TF(t)Ã—IDF(t)\n",
    "\n",
    "The TF-IDF score is high for terms that are common in a particular document but rare across other documents in the corpus, thereby capturing terms that are potentially more informative.\n",
    "\n",
    "The numerical vectors obtained through BoW and TF-IDF can then be used as features for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "an             1\n",
       "data           2\n",
       "field          1\n",
       "interesting    1\n",
       "is             3\n",
       "language       1\n",
       "learning       1\n",
       "love           1\n",
       "machine        1\n",
       "of             1\n",
       "programming    1\n",
       "python         1\n",
       "science        2\n",
       "subset         1\n",
       "versatile      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_bow > 0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10536051565782634"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>data</th>\n",
       "      <th>field</th>\n",
       "      <th>interesting</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>of</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>science</th>\n",
       "      <th>subset</th>\n",
       "      <th>versatile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         an      data     field  interesting        is  language  learning  \\\n",
       "0  0.000000  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000     0.000000  1.223144  1.916291  0.000000   \n",
       "2  1.916291  1.510826  1.916291     1.916291  1.223144  0.000000  0.000000   \n",
       "3  0.000000  1.510826  0.000000     0.000000  1.223144  0.000000  1.916291   \n",
       "\n",
       "       love   machine        of  programming    python   science    subset  \\\n",
       "0  1.916291  0.000000  0.000000     1.916291  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000     0.000000  1.916291  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000     0.000000  0.000000  1.510826  0.000000   \n",
       "3  0.000000  1.916291  1.916291     0.000000  0.000000  1.510826  1.916291   \n",
       "\n",
       "   versatile  \n",
       "0   0.000000  \n",
       "1   1.916291  \n",
       "2   0.000000  \n",
       "3   0.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Term Frequency (TF)\n",
    "tf = df_bow.values\n",
    "\n",
    "# Document Frequency (DF)\n",
    "df = (df_bow > 0).sum(axis=0)\n",
    "\n",
    "# Inverse Document Frequency (IDF)\n",
    "idf = np.log((len(documents) + 1) / (df + 1)) + 1  # Adding 1 to avoid division by zero and following sklearn's formula\n",
    "\n",
    "df = np.array(df)\n",
    "idf = np.array(idf)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_manual = pd.DataFrame(tf * idf, columns=df_bow.columns)\n",
    "tfidf_manual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a transformer class that performs both the BoW and TF-IDF for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>data</th>\n",
       "      <th>field</th>\n",
       "      <th>interesting</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>of</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>science</th>\n",
       "      <th>subset</th>\n",
       "      <th>versatile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         an      data     field  interesting        is  language  learning  \\\n",
       "0  0.000000  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000     0.000000  1.223144  1.916291  0.000000   \n",
       "2  1.916291  1.510826  1.916291     1.916291  1.223144  0.000000  0.000000   \n",
       "3  0.000000  1.510826  0.000000     0.000000  1.223144  0.000000  1.916291   \n",
       "\n",
       "       love   machine        of  programming    python   science    subset  \\\n",
       "0  1.916291  0.000000  0.000000     1.916291  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000     0.000000  1.916291  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000     0.000000  0.000000  1.510826  0.000000   \n",
       "3  0.000000  1.916291  1.916291     0.000000  0.000000  1.510826  1.916291   \n",
       "\n",
       "   versatile  \n",
       "0   0.000000  \n",
       "1   1.916291  \n",
       "2   0.000000  \n",
       "3   0.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(norm=None)  # Disable L2 normalization for comparison\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(documents)\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
    "df_tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models on the IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the IMDB movie reviews dataset for a binary classification task, where the goal is to classify movie reviews as either positive or negative. We apply both BoW and TF-IDF encoding techniques to the text data, train a Logistic Regression model, and evaluate the model's performance using a classification report. The classification report provides key metrics such as precision, recall, and F1-score, giving a comprehensive view of how well the model performs for each class (positive and negative reviews) under both encoding schemes. \n",
    "\n",
    "Experimenting with different text encoding techniques is a crucial step in handling text classification problems, as the choice of encoding can significantly impact the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (2.14.5)\n",
      "Requirement already satisfied: aiohttp in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (0.17.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (1.21.2)\n",
      "Requirement already satisfied: pandas in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: packaging in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: multiprocess in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: xxhash in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from datasets) (4.63.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.10)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: filelock in /home/nocampo/.local/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/nocampo/miniconda3/envs/phd-corpora/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.81k/7.81k [00:00<00:00, 4.61MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.0M/21.0M [00:08<00:00, 2.35MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:07<00:00, 2.82MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.0M/42.0M [00:16<00:00, 2.60MB/s]\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:32<00:00, 10.78s/it]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 3113.81it/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 350846.86 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 350118.20 examples/s]\n",
      "Generating unsupervised split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:00<00:00, 391035.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the IMDB dataset using Hugging Face's datasets library\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# The data is split into train, test, and unsupervised (which we won't use here)\n",
    "X_train, y_train = dataset['train']['text'], dataset['train']['label']\n",
    "X_test, y_test = dataset['test']['text'], dataset['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Bag of Words):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87     12500\n",
      "           1       0.87      0.86      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encoding and Model Training: Bag of Words\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n",
    "model_bow = LogisticRegression(max_iter=1000)\n",
    "model_bow.fit(X_train_bow, y_train)\n",
    "y_pred_bow = model_bow.predict(X_test_bow)\n",
    "print(f'Classification Report (Bag of Words):\\n{classification_report(y_test, y_pred_bow)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 74849)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     12500\n",
      "           1       0.88      0.88      0.88     12500\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encoding and Model Training: TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "model_tfidf = LogisticRegression(max_iter=1000)\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "print(f'Classification Report (TF-IDF):\\n{classification_report(y_test, y_pred_tfidf)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a261fe3232d26f5c8a3b17afd155c889a6f52aad30d4dff29769029555a05e9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('phd-corpora')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
